# ğŸ”¥ Spark OOM & Failure Prevention

### Senior Engineer Pre-Flight Checklist

> Think of this as **mandatory pre-flight checks** before running any Spark job.

---

## 1ï¸âƒ£ Data Shape Check (Most Skipped â€” Most Critical)

**Before writing a single line of code, answer these questions:**

* What is the **largest table size**?
* What is the **join key cardinality**?
* Is there **data skew**?
* Are there **hot keys**?
* What is the **row width** (number and size of columns)?

> âš ï¸ If you skip this step, **AQE will not save you**.

---

## 2ï¸âƒ£ Partitioning Checklist (Where Most OOMs Begin)

### Rule of Thumb

* **Each task should process 100â€“300 MB max**

### Recommended

```python
df = df.repartition("join_key")
```

### ğŸš« Avoid at All Costs

```python
df.repartition(1)  # ğŸ’€ Guaranteed pain
```

### ğŸš© Red Flags

* Very few partitions
* Extremely wide rows
* `groupBy` on low-cardinality columns

---

## 3ï¸âƒ£ Join Safety Checklist (OOM Factory)

### Always Ask

* Which side is **smaller**?
* Can the smaller side **grow unexpectedly**?

### Safe Default

```python
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "-1")
```

### Explicit & Safe

```python
df_big.join(broadcast(df_small), "id")
```

> ğŸ‘‰ **You decide the join strategy â€” not Spark.**

---

## 4ï¸âƒ£ Aggregation Safety (Silent Killer)

### ğŸš« Dangerous

```python
df.groupBy("country").count()  # INDIA dominates
```

### âœ… Safer Options

* Use **salting**
* Increase shuffle partitions

```python
spark.conf.set("spark.sql.shuffle.partitions", 400)
```

---

## 5ï¸âƒ£ Cache Discipline (Avoids Many Failures Alone)

### Cache **Only If**

* âœ” Data is reused multiple times
* âœ” Data fits comfortably in memory

### ğŸš« NEVER

* Cache raw large datasets
* Cache before `filter` / `select`

### âœ… Correct Pattern

```python
df_filtered = df.filter(...).select(...)
df_filtered.cache()
```

---

## 6ï¸âƒ£ UDF & Pandas UDF Rules (High-Risk Zone)

### Avoid When Possible

* Pandas UDFs on large partitions
* Python object-heavy logic

### If You Must

* Reduce partition size
* Use Arrow **intentionally**, not blindly

---

## 7ï¸âƒ£ Driver Safety (Very Common Mistake)

### ğŸš« NEVER on Large Data

```python
df.collect()
df.toPandas()
```

### âœ… Safe Alternative

```python
df.limit(1000).toPandas()
```

---

## 8ï¸âƒ£ Spark UI: How to Find the Real Root Cause

### When a Job Fails

1. Open **Spark UI**
2. Go to **Stages**
3. Identify the stage with:

   * One task much slower than others
   * Extremely large input size

### Inspect

* Shuffle Read / Write
* Spill (Memory â†’ Disk)

> ğŸ‘‰ This tells you **exactly** what went wrong.

---

## 9ï¸âƒ£ AQE: Use It Correctly

### Let AQE Handle

* âœ” Minor skew
* âœ” Partition coalescing
* âœ” Join switching

### YOU Must Handle

* âŒ Extreme skew
* âŒ Bad partitioning
* âŒ Cache misuse
* âŒ UDF memory abuse

---

## ğŸ”‘ Golden Rules (Burn These In)

* Shuffles create **stages**
* Stages create **tasks**
* Tasks consume **memory**
* **One bad task can kill the entire job**

---

## ğŸ§  Mindset Shift

**Junior Engineer thinks:**

> â€œSpark failed.â€

**Senior Engineer thinks:**

> â€œWhich task processed too much data?â€

---

## ğŸ“Œ Final Takeaway

> **AQE optimizes execution plans.
> Data engineers design safety.**
