ğŸ”¥ Spark OOM & Failure Prevention â€“ Senior Engineer Checklist
Think of this as â€œpre-flight checksâ€ before you run a Spark job.

1ï¸âƒ£ Before writing code: DATA SHAPE CHECK (most skipped)
Ask these 5 questions:
1. What is the largest table size?
2. What is the join key cardinality?
3. Is data skewed?
4. Are there hot keys?
5. What is the row width (number & size of columns)?
ğŸ‘‰ If you skip this, AQE wonâ€™t save you.

2ï¸âƒ£ Partitioning checklist (most OOMs start here)
Rule of thumb:
Each task should process 100â€“300 MB max
What YOU must do:

df = df.repartition("join_key")
Avoid:

df.repartition(1)   # ğŸ’€

Red flags:
* Very few partitions
* Very wide rows
* groupBy on low-cardinality column

3ï¸âƒ£ Join safety checklist (OOM factory)
Always ask:
* Which side is smaller?
* Can it grow unexpectedly?
Safe defaults:

spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "-1")
Then explicitly:

df_big.join(broadcast(df_small), "id")
ğŸ‘‰ You decide, not Spark.

4ï¸âƒ£ Aggregation safety (silent killer)
Dangerous:

df.groupBy("country").count()  # country = INDIA dominates
Safer:
* Use salting
* Or increase shuffle partitions

spark.conf.set("spark.sql.shuffle.partitions", 400)

5ï¸âƒ£ Cache discipline (this alone avoids many failures)
Only cache if:
âœ” Data reused multiple timesâ€¨âœ” Data fits in memory
NEVER:
* Cache raw large datasets
* Cache before filter / select

df_filtered = df.filter(...).select(...)
df_filtered.cache()

6ï¸âƒ£ UDF / Pandas UDF rules (OOM hotspot)
Avoid if possible:
* Pandas UDF with large partitions
* Python object-heavy logic
If needed:
* Reduce partition size
* Use Arrow wisely

7ï¸âƒ£ Driver safety (very common mistake)
NEVER on large data:

df.collect()
df.toPandas()
Instead:

df.limit(1000).toPandas()

8ï¸âƒ£ Spark UI: how to find root cause (VERY IMPORTANT)
When job fails:
1. Go to Stages
2. Find stage with:
    * One task much slower
    * Huge input size
3. Check:
    * Shuffle Read / Write
    * Spill (memory â†’ disk)
ğŸ‘‰ That tells you exactly what went wrong.

9ï¸âƒ£ AQE: how to use it correctly
Let AQE handle:
âœ” Minor skewâ€¨âœ” Partition coalescingâ€¨âœ” Join switching
YOU handle:
âŒ Extreme skewâ€¨âŒ Bad partitioningâ€¨âŒ Cache misuseâ€¨âŒ UDF memory

ğŸ”‘ Golden rules (print these mentally)
â€¢ Shuffles cause stagesâ€¨â€¢ Stages cause tasksâ€¨â€¢ Tasks consume memoryâ€¨â€¢ One bad task can kill a job

ğŸ§  Senior engineer mindset shift (important)
Junior thinks:
â€œSpark failedâ€
Senior thinks:
â€œWhich task processed too much data?â€

ğŸ“Œ Final one-line summary (lock this in)
AQE optimizes plans,â€¨but data engineers design safety.
